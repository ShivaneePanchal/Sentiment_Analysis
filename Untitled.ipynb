{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentiment                                            reviews\n",
      "count      25000                                              25000\n",
      "unique         3                                              24801\n",
      "top            1  Loved today's show!!! It was a variety and not...\n",
      "freq       12500                                                  5\n",
      "       sentiment                                            reviews\n",
      "count      25000                                              25000\n",
      "unique         2                                              24904\n",
      "top            0  You do realize that you've been watching the E...\n",
      "freq       12500                                                  3\n",
      "0        sentiment\n",
      "1                1\n",
      "2                1\n",
      "3                1\n",
      "4                1\n",
      "           ...    \n",
      "24995            0\n",
      "24996            0\n",
      "24997            0\n",
      "24998            0\n",
      "24999            0\n",
      "Name: sentiment, Length: 25000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Python packages to import \n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plot \n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "#Getting CSV file and setting column values\n",
    "df=pd.read_csv('/Users/shivaneeprajapati/Desktop/TakenMind/Spotle.ai/SentimentAnalysis_IMDB/Imdb_Sentiment_Analyzer_Arnav_Raina/movie_review_data.csv',\n",
    "               sep=\",\", names=['sentiment','reviews'])\n",
    "\n",
    "df.loc[:, ['sentiment', 'reviews']] = df[['reviews', 'sentiment']].to_numpy()\n",
    "#Splitting csv into testing and traing data\n",
    "traindf =df.iloc[0:25000]\n",
    "testdf = df.iloc[25000:50000]\n",
    "#Getting info on training and testing data\n",
    "print(traindf.describe())\n",
    "print(testdf.describe())\n",
    "print(traindf.sentiment)\n",
    "\n",
    "#Ignore user and Package Warning if any\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lem_tokens(tokens, lemmatizer):\n",
    "    lemmetized = []\n",
    "    for item in tokens:\n",
    "        lemmetized.append(lemmatizer.lemmatize(item))\n",
    "    return lemmetized\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '',text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    stems = lem_tokens(tokens, lemmatizer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
